
# ğŸ“° News RAG Chatbot -- Backend API


## ğŸ—ï¸ Architecture Diagram

![RAG Backend Architecture](../Advanced-RAG.png)

**Flow:**
- News is fetched and chunked by `ingest.js` from multiple RSS feeds.
- Chunks are embedded via Jina and stored in Qdrant.
- User queries hit the Express API, which retrieves context from Qdrant, manages chat history in Redis, and generates answers using Gemini.

A production-ready backend service for a **Retrieval-Augmented
Generation (RAG) News Chatbot**, built using Node.js, Qdrant, Redis,
Jina Embeddings, and Google Gemini. It ingests real-time news, stores
semantic vectors, retrieves context, and generates grounded AI
responses.

## â­ Overview

This backend handles:

-   Fetching and chunking news articles from RSS feeds\
-   Generating embedding vectors using **Jina Embeddings v2**\
-   Storing and searching vectors in **Qdrant**\
-   Maintaining chat sessions using **Redis**\
-   Generating context-aware responses using **Google Gemini**\
-   Exposing REST APIs for chat interactions and session management

## ğŸ› ï¸ Tech Stack

  Component          Technology
  ------------------ ------------------------
  Runtime            Node.js (Express.js)
  LLM                Google Gemini
  Embeddings         Jina Embeddings v2
  Vector Database    Qdrant (Docker)
  Cache / Sessions   Redis (Docker)
  Data Source        RSS Feeds (TechCrunch, BBC, Guardian, Al Jazeera)

## ğŸš€ Features

### ğŸ”¹ RAG Pipeline

-   Fetches news from RSS\
-   Chunks content\
-   Generates embeddings\
-   Stores semantic vectors in Qdrant

### ğŸ”¹ Context-Aware Chat

-   Retrieves top-k relevant articles\
-   Sends grounded context to Gemini\
-   Produces fact-based answers

### ğŸ”¹ Session Management

-   Chat history stored per session in Redis\
-   TTL: **24 hours**\
-   Automatic cleanup of inactive sessions

## ğŸ“‚ Project Structure

    rag-backend/
    â”‚â”€â”€ ingest.js         # News ingestion and vectorization
    â”‚â”€â”€ server.js         # Main Express API server
    â”‚â”€â”€ query.js          # Standalone query/test script
    â”‚â”€â”€ .env
    â”‚â”€â”€ package.json
    â””â”€â”€ README.md

## ğŸ—ï¸ Demo Screenshot
![Demo Screenshot](../rag-chatbot-demo.JPG)

## âš™ï¸ Installation & Setup

### 1ï¸âƒ£ Prerequisites

-   Node.js v18+\
-   Docker installed

### 2ï¸âƒ£ Start Required Services

#### Start Qdrant

``` bash
docker run -d -p 6333:6333   -v $(pwd)/qdrant_storage:/qdrant/storage   qdrant/qdrant
```

#### Start Redis

``` bash
docker run -d -p 6379:6379 redis
```

### 3ï¸âƒ£ Install Dependencies

``` bash
npm install
```

### 4ï¸âƒ£ Environment Variables

Create a `.env` file:

  JINA_API_KEY=your_jina_api_key
  GEMINI_API_KEY=your_gemini_api_key

### 5ï¸âƒ£ Ingest News Articles

```bash
node ingest.js
```

This will fetch and chunk news from multiple RSS feeds (TechCrunch, BBC, Guardian, Al Jazeera), generate embeddings for each chunk, and store them in Qdrant. Each article is split into overlapping chunks for better retrieval.

Expected output:

  âœ… Ingestion complete! Articles indexed.

### 6ï¸âƒ£ Start the Server

```bash
node server.js
```

Server runs at:
**http://localhost:3000**

## ğŸ“¡ API Endpoints

### **POST /api/chat**

Send a user message and session ID. The backend retrieves relevant news chunks, builds a context, and generates a Gemini-based answer. Chat history is stored in Redis (last 10 messages for context).

Request:

```json
{
  "sessionId": "uuid",
  "message": "Whatâ€™s the latest in AI?"
}
```

Response:

```json
{
  "reply": "Here is the latest update..."
}
```

### **GET /api/session/:sessionId**

Retrieve chat history for a session (returns all messages, most recent first).

### **DELETE /api/session/:sessionId**

Clear chat history for a session.

## ğŸ§  Design Decisions & Improvements

- **Multi-source news ingestion**: Now fetches from TechCrunch, BBC, Guardian, Al Jazeera.
- **Chunked context**: Articles are split into overlapping chunks for better semantic search.
- **Improved context retrieval**: Top 5 relevant chunks are retrieved and used for Gemini prompts.
- **Session management**: Last 10 messages are used for context, stored in Redis with 24h TTL.
- **Error handling**: Graceful fallback if no relevant news is found.
- **Standalone query.js**: For testing queries and context retrieval without running the server.

## ğŸ“¦ Dependencies

Key NPM packages:

- express, cors, dotenv, axios
- @qdrant/js-client-rest
- @google/generative-ai
- ioredis
- rss-parser
- uuid

See package.json for full list.

## ğŸ“ Summary

This backend provides a clean, modular, and scalable RAG system using modern AI tools. It supports multi-source news ingestion, chunked semantic search, grounded chat responses, and efficient session handling.
